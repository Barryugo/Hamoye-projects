{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPool2D\nfrom tensorflow.keras.applications import ResNet50, VGG16\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.model_selection import train_test_split\n\nimport cv2\nimport os\nfrom tqdm import tqdm\n\nimport time\n\nfrom os import listdir\nimport csv","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:56:40.076429Z","iopub.execute_input":"2023-01-16T14:56:40.076863Z","iopub.status.idle":"2023-01-16T14:56:47.263068Z","shell.execute_reply.started":"2023-01-16T14:56:40.076767Z","shell.execute_reply":"2023-01-16T14:56:47.262077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nfrom time import time\nfrom glob import glob\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\nimport dill as pickle\n\nfrom plotly import graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nimport cv2\n\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import fbeta_score, confusion_matrix\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms as T, models\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR\n!pip install -q torchsummary --user\nfrom torchsummary import summary\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:56:47.268250Z","iopub.execute_input":"2023-01-16T14:56:47.271848Z","iopub.status.idle":"2023-01-16T14:57:02.852567Z","shell.execute_reply.started":"2023-01-16T14:56:47.271808Z","shell.execute_reply":"2023-01-16T14:57:02.851194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.seed(101)\nnp.random.seed(101)\ntorch.manual_seed(101);","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:02.854932Z","iopub.execute_input":"2023-01-16T14:57:02.855690Z","iopub.status.idle":"2023-01-16T14:57:02.863250Z","shell.execute_reply.started":"2023-01-16T14:57:02.855646Z","shell.execute_reply":"2023-01-16T14:57:02.862330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ../input/planets-dataset/planet/planet","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:02.866506Z","iopub.execute_input":"2023-01-16T14:57:02.866856Z","iopub.status.idle":"2023-01-16T14:57:03.843337Z","shell.execute_reply.started":"2023-01-16T14:57:02.866821Z","shell.execute_reply":"2023-01-16T14:57:03.841922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"../input/planets-dataset/planet/planet/\"\npath_train = os.path.join(path, \"train-jpg\")\npath_test = os.path.join(path, \"test-jpg\")\nprint(\n    f\"train files: {len(os.listdir(path_train))}, \"\n    f\"test files: {len(os.listdir(path_test))}\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:03.845185Z","iopub.execute_input":"2023-01-16T14:57:03.845948Z","iopub.status.idle":"2023-01-16T14:57:04.943269Z","shell.execute_reply.started":"2023-01-16T14:57:03.845892Z","shell.execute_reply":"2023-01-16T14:57:04.942138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_class = os.path.join(path, \"train_classes.csv\")\ndf_class = pd.read_csv(path_class)\nprint(df_class.shape)\ndf_class.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:04.944672Z","iopub.execute_input":"2023-01-16T14:57:04.945598Z","iopub.status.idle":"2023-01-16T14:57:05.015066Z","shell.execute_reply.started":"2023-01-16T14:57:04.945557Z","shell.execute_reply":"2023-01-16T14:57:05.014085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_class[\"list_tags\"] = df_class.tags.str.split(\" \")\nrow_tags = df_class.list_tags.values\ntags = [tag for row in row_tags for tag in row]\ncounter_tags = Counter(tags)\ndf_tags = pd.DataFrame(\n    {\"tag\": counter_tags.keys(), \"total\": counter_tags.values()}\n).sort_values(\"total\")\n\nfig = px.bar(df_tags, x=\"total\", y=\"tag\", orientation=\"h\", \n             color=\"total\",\n)\nfig.update_layout(title=\"Class distribution\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:05.016660Z","iopub.execute_input":"2023-01-16T14:57:05.017359Z","iopub.status.idle":"2023-01-16T14:57:05.787750Z","shell.execute_reply.started":"2023-01-16T14:57:05.017321Z","shell.execute_reply":"2023-01-16T14:57:05.786808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RARE_CLASSES = [\n    \"bare_ground\", \"selective_logging\", \"artisinal_mine\", \"blooming\", \"slash_burn\", \"blow_down\", \"conventional_mine\"\n]","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:05.789130Z","iopub.execute_input":"2023-01-16T14:57:05.789605Z","iopub.status.idle":"2023-01-16T14:57:05.795753Z","shell.execute_reply.started":"2023-01-16T14:57:05.789566Z","shell.execute_reply":"2023-01-16T14:57:05.794685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_tags = list(set(tags))\nN_tags = len(all_tags)\nfig, axes = plt.subplots(4, (N_tags//4)+1, figsize=(20, 20))\nfor idx, tag in enumerate(all_tags):\n    filename = df_class.loc[df_class.tags.str.contains(tag)].image_name.values[0]\n    img = cv2.imread(os.path.join(path_train, filename+\".jpg\"))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    idx_col = idx // 4\n    idx_row = idx % 4\n    axes[idx_row][idx_col].set_title(tag)\n    axes[idx_row][idx_col].imshow(img)\naxes[1][-1].remove()\naxes[2][-1].remove()\naxes[3][-1].remove()","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:05.797337Z","iopub.execute_input":"2023-01-16T14:57:05.798071Z","iopub.status.idle":"2023-01-16T14:57:09.258312Z","shell.execute_reply.started":"2023-01-16T14:57:05.798034Z","shell.execute_reply":"2023-01-16T14:57:09.257078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_img(path_file):\n    img = cv2.imread(path_file)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (100, 100), cv2.INTER_LINEAR).astype(float)\n    img = cv2.normalize(img, None, 0.0, 1.0, cv2.NORM_MINMAX)\n    img = img.reshape(1, -1)\n    return img","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:09.262447Z","iopub.execute_input":"2023-01-16T14:57:09.262949Z","iopub.status.idle":"2023-01-16T14:57:09.270185Z","shell.execute_reply.started":"2023-01-16T14:57:09.262916Z","shell.execute_reply":"2023-01-16T14:57:09.269127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames = df_class.image_name.sample(600).values\npath_files = [os.path.join(path_train, filename+\".jpg\") for filename in filenames]\nX_train_sample = np.vstack([load_img(path_file) for path_file in path_files])\nX_train_sample.shape   \n","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:09.271839Z","iopub.execute_input":"2023-01-16T14:57:09.272662Z","iopub.status.idle":"2023-01-16T14:57:14.317860Z","shell.execute_reply.started":"2023-01-16T14:57:09.272623Z","shell.execute_reply":"2023-01-16T14:57:14.316851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne = TSNE(\n    n_components=2,\n    init=\"random\",\n    random_state=101,\n    method=\"barnes_hut\",\n    n_iter=500,\n    verbose=2,\n)\nX_embedded = tsne.fit_transform(X_train_sample)\nX_embedded.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:14.319371Z","iopub.execute_input":"2023-01-16T14:57:14.320148Z","iopub.status.idle":"2023-01-16T14:57:16.957684Z","shell.execute_reply.started":"2023-01-16T14:57:14.320108Z","shell.execute_reply":"2023-01-16T14:57:16.956593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_img(path_file, h, w):\n    img = cv2.imread(path_file)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (h*2, w*2), cv2.INTER_LINEAR)\n    return img","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:16.959400Z","iopub.execute_input":"2023-01-16T14:57:16.959787Z","iopub.status.idle":"2023-01-16T14:57:16.965998Z","shell.execute_reply.started":"2023-01-16T14:57:16.959749Z","shell.execute_reply":"2023-01-16T14:57:16.965076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size_img = 1000\noffset_img = 50\nh = w = int(offset_img / 2)\n\nX_scaled = (X_embedded - X_embedded.min(0)) / (X_embedded.max(0) - X_embedded.min(0))\nX_scaled = (X_scaled * size_img).astype(int)\nX_scaled = np.clip(X_scaled, offset_img, size_img-offset_img)\n\nimg_tsne = np.ones((size_img+2*offset_img, size_img+2*offset_img, 3), dtype=np.uint8) * 255\nfor idx in range(X_scaled.shape[0]):\n    x, y = X_scaled[idx][0], X_scaled[idx][1]\n    img = fetch_img(path_files[idx], h, w)\n    img_tsne[x-w:x+w, y-h:y+h, :] = img\n\nfig = plt.figure(figsize=(12, 12))\nplt.imshow(img_tsne);\nplt.axis(\"off\");","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:16.967477Z","iopub.execute_input":"2023-01-16T14:57:16.968477Z","iopub.status.idle":"2023-01-16T14:57:18.702744Z","shell.execute_reply.started":"2023-01-16T14:57:16.968440Z","shell.execute_reply":"2023-01-16T14:57:18.701930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transforms():\n    transform_train = T.Compose([\n      T.ToPILImage(),\n      T.Resize(224),\n      T.ToTensor(),\n      T.Normalize(\n          mean=[0.485, 0.456, 0.406],\n          std=[0.229, 0.224, 0.225],\n      )\n    ])\n    transform_val = T.Compose([\n      T.ToPILImage(),\n      T.Resize(224),\n      T.ToTensor(),\n      T.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n      )\n    ])\n    return transform_train, transform_val","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:18.704086Z","iopub.execute_input":"2023-01-16T14:57:18.704676Z","iopub.status.idle":"2023-01-16T14:57:18.714418Z","shell.execute_reply.started":"2023-01-16T14:57:18.704641Z","shell.execute_reply":"2023-01-16T14:57:18.713422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AmazonDatasetError(Exception):\n    pass\n\n\nclass AmazonDataset(Dataset):\n    def __init__(self, df, ohe_tags, transform, path, is_train=True, idx_tta=None):\n        super().__init__()\n        self.df = df\n        self.ohe_tags = ohe_tags\n        self.transform = transform\n        if isinstance(path, str):\n            self.paths = [path]\n        elif isinstance(path, (list, tuple)):\n            self.paths = path\n        else:\n            raise AmazonDatasetError(f\"Path type must be str, list or tuple, got: {type(path)}\")\n        self.is_train = is_train\n        if not is_train:\n            if not idx_tta in list(range(6)):\n                raise AmazonDatasetError(\n                    f\"In test mode, 'idx_tta' must be an int belonging to [0, 5], got: {repr(idx_tta)}\"\n                )\n            self.idx_tta = idx_tta\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        filename = self.df.iloc[idx].image_name + \".jpg\"\n        for path in self.paths:\n            if filename in os.listdir(path):\n                file_path = os.path.join(path, filename)\n                break\n        else:\n            raise AmazonDatasetError(f\"Can't fetch {filename} among {self.paths}\")\n        img = cv2.imread(file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        label = self.ohe_tags[idx]\n        return img, label\n\n    def collate_fn(self, batch):\n        imgs, labels = [], []\n        for (img, label) in batch:\n            img = self.custom_augment(img)\n            img = torch.tensor(img)\n            img = img.permute(2, 0, 1)\n            img = self.transform(img)\n            imgs.append(img[None])\n            labels.append(label)\n        imgs = torch.cat(imgs).float().to(device)\n        labels = torch.tensor(labels).float().to(device)\n        return imgs, labels\n\n    def load_img(self, idx, ax=None):\n        img, ohe_label = self[idx]\n        label = self.df.iloc[idx].tags\n        title = f\"{label} - {ohe_label}\"\n        if ax is None:\n            plt.imshow(img)\n            plt.title(title)\n        else:\n            ax.imshow(img)\n            ax.set_title(title)\n    \n    def custom_augment(self, img):\n        \"\"\"\n        Discrete rotation and horizontal flip.\n        Random during training and non random during testing for TTA.\n        Not implemented in torchvision.transforms, hence this function.\n        \"\"\"\n        choice = np.random.randint(0, 6) if self.is_train else self.idx_tta\n        if choice == 0:\n            # Rotate 90\n            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_CLOCKWISE)\n        if choice == 1:\n            # Rotate 90 and flip horizontally\n            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_CLOCKWISE)\n            img = cv2.flip(img, flipCode=1)\n        if choice == 2:\n            # Rotate 180\n            img = cv2.rotate(img, rotateCode=cv2.ROTATE_180)\n        if choice == 3:\n            # Rotate 180 and flip horizontally\n            img = cv2.rotate(img, rotateCode=cv2.ROTATE_180)\n            img = cv2.flip(img, flipCode=1)\n        if choice == 4:\n            # Rotate 90 counter-clockwise\n            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_COUNTERCLOCKWISE)\n        if choice == 5:\n            # Rotate 90 counter-clockwise and flip horizontally\n            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_COUNTERCLOCKWISE)\n            img = cv2.flip(img, flipCode=1)\n        return img","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:18.715929Z","iopub.execute_input":"2023-01-16T14:57:18.716539Z","iopub.status.idle":"2023-01-16T14:57:18.737162Z","shell.execute_reply.started":"2023-01-16T14:57:18.716503Z","shell.execute_reply":"2023-01-16T14:57:18.736323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data(df_train, df_val):\n\n    encoder = MultiLabelBinarizer()\n    ohe_tags_train = encoder.fit_transform(df_train.list_tags.values)\n    ohe_tags_val = encoder.transform(df_val.list_tags.values)\n\n    transform_train, transform_val = get_transforms()\n    ds_train = AmazonDataset(df_train, ohe_tags_train, transform_train, path=path_train)\n    ds_val = AmazonDataset(df_val, ohe_tags_val, transform_val, path=path_train)\n\n    dl_train = DataLoader(\n      ds_train,\n      batch_size=256,\n      shuffle=True,\n      collate_fn=ds_train.collate_fn\n    )\n    dl_val = DataLoader(\n      ds_val,\n      batch_size=256,\n      shuffle=True,\n      collate_fn=ds_val.collate_fn\n    )\n\n    return ds_train, ds_val, dl_train, dl_val, encoder","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:18.739390Z","iopub.execute_input":"2023-01-16T14:57:18.739660Z","iopub.status.idle":"2023-01-16T14:57:18.755651Z","shell.execute_reply.started":"2023-01-16T14:57:18.739635Z","shell.execute_reply":"2023-01-16T14:57:18.754467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_val = train_test_split(df_class, test_size=.2)\n\nds_train, ds_val, dl_train, dl_val, encoder = get_data(df_train, df_val)\n\nimgs, labels = next(iter(dl_train))\nimgs.shape, labels.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:18.757428Z","iopub.execute_input":"2023-01-16T14:57:18.757799Z","iopub.status.idle":"2023-01-16T14:57:30.156129Z","shell.execute_reply.started":"2023-01-16T14:57:18.757764Z","shell.execute_reply":"2023-01-16T14:57:30.154580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_train.load_img(5)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:30.157444Z","iopub.execute_input":"2023-01-16T14:57:30.158265Z","iopub.status.idle":"2023-01-16T14:57:30.439402Z","shell.execute_reply.started":"2023-01-16T14:57:30.158225Z","shell.execute_reply":"2023-01-16T14:57:30.438481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    model = models.resnet18(pretrained=True)\n    for param in model.parameters():\n        param.require_grad = False\n    model.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n    model.fc = nn.Sequential(\n      nn.Flatten(),\n      nn.Linear(512, 128), # 512 for resnet18 or 2048 for resnet 50\n      nn.ReLU(inplace=True),\n      nn.Dropout(.2),\n      nn.Linear(128, 17),\n      nn.Sigmoid()\n    )\n    optimizer = Adam(model.parameters(), lr=1e-4)\n    loss_fn = nn.BCELoss()\n\n    return model.to(device), optimizer, loss_fn","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:30.440806Z","iopub.execute_input":"2023-01-16T14:57:30.441118Z","iopub.status.idle":"2023-01-16T14:57:30.459086Z","shell.execute_reply.started":"2023-01-16T14:57:30.441092Z","shell.execute_reply":"2023-01-16T14:57:30.455744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_batch(X, Y, model, loss_fn, optimizer):\n    model.train()\n    optimizer.zero_grad()\n    Y_hat = model(X)\n    batch_loss = loss_fn(Y_hat, Y)\n    batch_loss.backward()\n    optimizer.step()\n    Y_hat = Y_hat.detach().float().cpu().numpy()\n    \n    return Y_hat, batch_loss.item()\n\n\n@torch.no_grad()\ndef compute_val_loss(X, Y, model, loss_fn):\n    model.eval()\n    Y_hat = model(X)\n    batch_loss = loss_fn(Y_hat, Y)\n    Y_hat = Y_hat.detach().float().cpu().numpy()\n    \n    return Y_hat, batch_loss.item()","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:30.460389Z","iopub.execute_input":"2023-01-16T14:57:30.460746Z","iopub.status.idle":"2023-01-16T14:57:30.472521Z","shell.execute_reply.started":"2023-01-16T14:57:30.460710Z","shell.execute_reply":"2023-01-16T14:57:30.471302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(dl_train, dl_val, idx_fold):\n    model, optimizer, loss_fn = get_model()\n    lr_scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n\n    loss_train, loss_val = [], []\n    score_train, score_val = [], []\n\n    Y_hat_val = None\n    best_loss_val = np.inf\n\n    epochs = 3\n    for idx in range(epochs):\n        loss_train_epoch, loss_val_epoch = [], []\n        Y_hat_train_epoch, Y_hat_val_epoch = [], []\n        Y_train_epoch, Y_val_epoch = [], []\n\n        for X, Y in tqdm(dl_train, leave=False):\n            Y_hat, batch_loss = train_batch(X, Y, model, loss_fn, optimizer)\n            loss_train_epoch.append(batch_loss)\n            Y_hat_train_epoch.extend(Y_hat)\n            Y_train_epoch.extend(Y.detach().float().cpu().numpy())\n\n        for X, Y in tqdm(dl_val, leave=False):\n            Y_hat, batch_loss = compute_val_loss(X, Y, model, loss_fn)\n            loss_val_epoch.append(batch_loss)\n            Y_hat_val_epoch.extend(Y_hat)\n            Y_val_epoch.extend(Y.detach().float().cpu().numpy())\n                \n        avg_loss_train = np.mean(loss_train_epoch)\n        avg_loss_val = np.mean(loss_val_epoch)\n\n        Y_hat_train_epoch = np.array(Y_hat_train_epoch)\n        Y_hat_val_epoch = np.array(Y_hat_val_epoch)\n        Y_thresh_train_epoch = (Y_hat_train_epoch > .2).astype(float)\n        Y_thresh_val_epoch = (Y_hat_val_epoch > .2).astype(float)\n        Y_train_epoch = np.array(Y_train_epoch)\n        Y_val_epoch = np.array(Y_val_epoch)\n        \n        score_train_epoch = fbeta_score(Y_train_epoch, Y_thresh_train_epoch, beta=2, average=\"samples\")\n        score_val_epoch = fbeta_score(Y_val_epoch, Y_thresh_val_epoch, beta=2, average=\"samples\")\n               \n        # saving values for debugging\n        if avg_loss_val < best_loss_val:\n            best_loss_val = avg_loss_val\n            Y_hat_val = Y_hat_val_epoch\n            Y_thresh_val = Y_thresh_val_epoch\n            Y_val = Y_val_epoch\n            \n        loss_train.append(avg_loss_train)\n        loss_val.append(avg_loss_val)\n        score_train.append(score_train_epoch)\n        score_val.append(score_val_epoch)\n\n        print(\n            f\"epoch: {idx}/{epochs} -- train loss: {avg_loss_train}, \" \\\n            f\"val loss: {avg_loss_val}\" \\\n            f\" -- train fbeta_score: {score_train_epoch}, \" \\\n            f\"val fbeta_score: {score_val_epoch}\"\n        )\n        \n        lr_scheduler.step()\n\n    train_results = {\n        \"loss_train\": loss_train,\n        \"loss_val\": loss_val,\n        \"score_train\": score_train,\n        \"score_val\": score_val,\n        \"Y_hat_val\": Y_hat_val,\n        \"Y_thresh_val\": Y_thresh_val,\n        \"Y_val\": Y_val,\n    }\n        \n    torch.save(model, f\"resnet18_fold{idx_fold}.pth\")\n    pickle.dump(train_results, open(f\"train_results_fold{idx_fold}.pkl\", \"wb\"))","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:30.474890Z","iopub.execute_input":"2023-01-16T14:57:30.475944Z","iopub.status.idle":"2023-01-16T14:57:30.504941Z","shell.execute_reply.started":"2023-01-16T14:57:30.475894Z","shell.execute_reply":"2023-01-16T14:57:30.504046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for rare_class in RARE_CLASSES:\n    total_train = df_train.loc[df_train.tags.str.contains(rare_class)].shape[0]\n    total_val = df_val.loc[df_val.tags.str.contains(rare_class)].shape[0]\n    print(f\"train {rare_class}: {100 * total_train / df_train.shape[0]:.4f}% ({total_train})\")\n    print(f\"val {rare_class}: {100 * total_val / df_val.shape[0]:.4f}% ({total_val})\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:30.506532Z","iopub.execute_input":"2023-01-16T14:57:30.507135Z","iopub.status.idle":"2023-01-16T14:57:30.630566Z","shell.execute_reply.started":"2023-01-16T14:57:30.507099Z","shell.execute_reply":"2023-01-16T14:57:30.629372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(dl_train, dl_val, 0)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T14:57:30.632180Z","iopub.execute_input":"2023-01-16T14:57:30.632620Z","iopub.status.idle":"2023-01-16T15:37:22.846080Z","shell.execute_reply.started":"2023-01-16T14:57:30.632582Z","shell.execute_reply":"2023-01-16T15:37:22.844923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.load(\"resnet18_fold0.pth\")\ntrain_results = pickle.load(open(\"train_results_fold0.pkl\", \"rb\"))","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:37:22.847912Z","iopub.execute_input":"2023-01-16T15:37:22.848641Z","iopub.status.idle":"2023-01-16T15:37:22.905904Z","shell.execute_reply.started":"2023-01-16T15:37:22.848587Z","shell.execute_reply":"2023-01-16T15:37:22.904571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_train = train_results[\"loss_train\"]\nloss_val = train_results[\"loss_val\"]\nscore_train = train_results[\"score_train\"]\nscore_val = train_results[\"score_val\"]\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Loss\", \"Fbeta scores\"))\nfig.add_trace(\n    go.Scatter(\n        x=list(range(len(loss_train))),\n        y=loss_train,\n        name=\"loss_train\",\n    ),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(\n        x=list(range(len(loss_val))),\n        y=loss_val,\n        name=\"loss_val\",\n    ),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(\n        x=list(range(len(score_train))),\n        y=score_train,\n        name=\"score_train\",\n    ),\n    row=1, col=2\n)\nfig.add_trace(\n    go.Scatter(\n        x=list(range(len(score_val))),\n        y=score_val,\n        name=\"score_val\",\n    ),\n    row=1, col=2\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:37:22.907843Z","iopub.execute_input":"2023-01-16T15:37:22.908316Z","iopub.status.idle":"2023-01-16T15:37:22.967181Z","shell.execute_reply.started":"2023-01-16T15:37:22.908268Z","shell.execute_reply":"2023-01-16T15:37:22.966148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_hat_val = np.array(train_results[\"Y_hat_val\"])\nY_val = np.array(train_results[\"Y_val\"])\n\npos_probas, neg_probas = [], []\nfor class_, idx in encoder._cached_dict.items():\n    pos_probas.append(Y_hat_val[np.where(Y_val[:, idx] != 0), idx].mean())\n    neg_probas.append(Y_hat_val[np.where(Y_val[:, idx] == 0), idx].mean())\ngo.Figure([\n    go.Bar(x=list(encoder._cached_dict), y=pos_probas, name=\"Y_hat proba | Y = 1\"),\n    go.Bar(x=list(encoder._cached_dict), y=neg_probas, name=\"Y_hat proba | Y = 0\")\n]).show()","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:37:22.968539Z","iopub.execute_input":"2023-01-16T15:37:22.968891Z","iopub.status.idle":"2023-01-16T15:37:22.987603Z","shell.execute_reply.started":"2023-01-16T15:37:22.968863Z","shell.execute_reply":"2023-01-16T15:37:22.986689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_best_thresholds(Y_hat, Y):\n    N_tags = Y.shape[1]\n    best_threshs = [0.2] * N_tags\n    resolution = 100\n    for jdx in tqdm(range(N_tags)):\n        best_score = 0\n        #threshs = np.zeros_like(best_threshs)\n        threshs = best_threshs.copy()\n        for kdx in range(resolution):\n            kdx /= resolution\n            threshs[jdx] = kdx\n            Y_hat_thresh = (Y_hat > threshs).astype(float)\n            score = fbeta_score(Y, Y_hat_thresh, beta=2, average=\"samples\")\n            if score > best_score:\n                best_score = score\n                best_threshs[jdx] = kdx\n    \n    global_best_score = fbeta_score(Y, (Y_hat > best_threshs).astype(float), beta=2, average=\"samples\")\n    print(f\"threshs: {best_threshs} -- best score: {global_best_score}\")\n    \n    return best_threshs","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:37:22.992776Z","iopub.execute_input":"2023-01-16T15:37:22.993079Z","iopub.status.idle":"2023-01-16T15:37:23.000247Z","shell.execute_reply.started":"2023-01-16T15:37:22.993052Z","shell.execute_reply":"2023-01-16T15:37:22.999151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshs = find_best_thresholds(Y_hat_val, Y_val)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:37:23.002103Z","iopub.execute_input":"2023-01-16T15:37:23.002478Z","iopub.status.idle":"2023-01-16T15:38:07.797733Z","shell.execute_reply.started":"2023-01-16T15:37:23.002428Z","shell.execute_reply":"2023-01-16T15:38:07.796528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_scores = {}\nclasses = encoder.classes_\nfor jdx in range(Y_val.shape[1]):\n    y_val = Y_val[:, jdx].ravel()\n    y_hat_val = (Y_hat_val[:, jdx].ravel() > threshs[jdx]).astype(float)\n    score = fbeta_score(y_val, y_hat_val, beta=2)\n    class_scores[classes[jdx]] = round(score, 4)\n\ndf_score = pd.DataFrame(dict(\n    label=list(class_scores.keys()), score=list(class_scores.values()),\n)).sort_values(\"score\", ascending=False)\nfig = px.bar(df_score, x=\"label\", y=\"score\", color=\"score\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:38:07.799259Z","iopub.execute_input":"2023-01-16T15:38:07.799866Z","iopub.status.idle":"2023-01-16T15:38:07.928903Z","shell.execute_reply.started":"2023-01-16T15:38:07.799825Z","shell.execute_reply":"2023-01-16T15:38:07.927878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(cols=5, rows=4)\nfor jdx in range(Y_val.shape[1]):\n    y_val = Y_val[:, jdx].ravel()\n    y_hat_val = (Y_hat_val[:, jdx].ravel() > threshs[jdx]).astype(float)\n    tn, fp, fn, tp = confusion_matrix(y_val, y_hat_val).ravel()\n    mat = np.array([[fn, tn], [tp, fp]])\n    col = jdx // 4+1\n    row = jdx % 4+1\n    fig.add_trace(\n        go.Heatmap(\n            z=mat, text=[[f\"fn: {fn}\", f\"tn: {tn}\"], [f\"tp: {tp}\", f\"fp: {fp}\"]], \n            texttemplate=\"%{text}\", colorscale='Viridis', name=encoder.classes_[jdx],\n            showscale=False\n        ),\n        col=col, row=row, \n    )\n    fig.update_xaxes(title=encoder.classes_[jdx], showticklabels=False, row=row, col=col)\n    fig.update_yaxes(showticklabels=False, row=row, col=col)\n    \n\nfig.update_layout(\n    width=1200, height=800, title=\"Confusion matrices\", \n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:38:07.930305Z","iopub.execute_input":"2023-01-16T15:38:07.931221Z","iopub.status.idle":"2023-01-16T15:38:08.572906Z","shell.execute_reply.started":"2023-01-16T15:38:07.931180Z","shell.execute_reply":"2023-01-16T15:38:08.571953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!echo $(ls ../input/planets-dataset/planet/planet/test-jpg | wc -l) + $(ls ../input/planets-dataset/test-jpg-additional/test-jpg-additional | wc -l)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:38:08.574626Z","iopub.execute_input":"2023-01-16T15:38:08.575053Z","iopub.status.idle":"2023-01-16T15:38:10.217504Z","shell.execute_reply.started":"2023-01-16T15:38:08.574990Z","shell.execute_reply":"2023-01-16T15:38:10.216391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_data(idx_tta):\n    path_test_table = \"../input/planets-dataset/planet/planet\"\n    path_test_file_1 = \"../input/planets-dataset/planet/planet/test-jpg\"\n    path_test_file_2 = \"../input/planets-dataset/test-jpg-additional/test-jpg-additional\"\n    file_count = len(os.listdir(path_test_file_1)) + len(os.listdir(path_test_file_2))\n    df_test = pd.read_csv(os.path.join(path_test_table, \"sample_submission.csv\"))\n    \n    assert df_test.shape[0] == file_count # sanity check\n    \n    ohe_tags_test = np.zeros((df_test.shape[0], 17))\n    _, transform_val = get_transforms()\n    ds_test = AmazonDataset(\n        df_test, ohe_tags_test, transform_val, path=[path_test_file_1, path_test_file_2],\n        is_train=False, idx_tta=idx_tta\n    )\n    dl_test = DataLoader(\n        ds_test, shuffle=False, batch_size=32, collate_fn=ds_test.collate_fn\n    )\n    \n    return dl_test, df_test","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:38:10.219752Z","iopub.execute_input":"2023-01-16T15:38:10.220189Z","iopub.status.idle":"2023-01-16T15:38:10.228094Z","shell.execute_reply.started":"2023-01-16T15:38:10.220145Z","shell.execute_reply":"2023-01-16T15:38:10.227029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef batch_predict(model, X):\n    model.eval()\n    Y = model(X)\n    return Y.detach().float().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:38:10.229561Z","iopub.execute_input":"2023-01-16T15:38:10.230170Z","iopub.status.idle":"2023-01-16T15:38:10.237965Z","shell.execute_reply.started":"2023-01-16T15:38:10.230132Z","shell.execute_reply":"2023-01-16T15:38:10.237056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_hat_test = []\nfor idx_tta in range(6):\n    Y_hat_test_tta = []\n    dl_test, df_test = get_test_data(idx_tta)\n    for X, _ in tqdm(dl_test):\n        Y_hat_test_batch = batch_predict(model, X)\n        Y_hat_test_tta.extend(Y_hat_test_batch)\n    Y_hat_test.append(Y_hat_test_tta)\nY_hat_test = np.mean(np.array(Y_hat_test), axis=0)\nY_hat_test = (Y_hat_test > threshs).astype(float)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T15:38:10.239511Z","iopub.execute_input":"2023-01-16T15:38:10.240126Z","iopub.status.idle":"2023-01-16T17:45:51.251836Z","shell.execute_reply.started":"2023-01-16T15:38:10.240089Z","shell.execute_reply":"2023-01-16T17:45:51.250682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_hat_test_inv = encoder.inverse_transform(Y_hat_test)\ntest_tags = []\nfor row in Y_hat_test_inv:\n    tags = \" \".join(row)\n    test_tags.append(tags)\n\ndf_test[\"tags\"] = test_tags\ndf_test.to_csv(\"MY_sample_submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T17:45:51.253664Z","iopub.execute_input":"2023-01-16T17:45:51.255303Z","iopub.status.idle":"2023-01-16T17:45:51.522076Z","shell.execute_reply.started":"2023-01-16T17:45:51.255255Z","shell.execute_reply":"2023-01-16T17:45:51.521117Z"},"trusted":true},"execution_count":null,"outputs":[]}]}